{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUQQisx09909q4r9kFvd4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majid0110/Neural-Network-for-XOR-gate/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJDK4IIg3giN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Steps:\n",
        "\n",
        "1.   Created Class names as 'NeuralNetwork' and initialized the weights\n",
        "2.   defined an activation function i.e sigmoid function i.e σ(x) = 1 / (1 + e ^ -x)\n",
        "3.  Then I used derivative of Sigmoid function that is neccessory for backword propegation for reducing error loss. i.e\n",
        "dy/dx (σ(x)) => σ ′(x)=σ(x)⋅(1−σ(x))\n",
        "\n",
        "4. Then I defined another function for feed farward, that will calculating the outputs of the neurons and theri outputs.  as a total I have 5 neurons . 2 for inputs, 2 hidden and 1 output neuron.\n",
        "\n",
        "\n",
        "\n",
        "*   General Formuls = x' = W1.input1 + W2.input2 + Bias\n",
        "*   Then I put the value of x' in the sigmoid function for output of hidden leyer i.e h = σ(x') = 1 / (1 + e ^ -x`)\n",
        "*  For out put, I considered values of hidden neuron and and applied same technique on it, applied same technique, i.e multiplied each by W and added bias to it to get my resulted output .\n",
        "* I used MSE (Mean Square Error) to calculate loss function i.e MSE = sum (actual -predicted)^2\n",
        "\n",
        "5. Now I started training my model by declearing a function for it as train it took data (dataset; in XOR case we have 4 possible combinations => [ [0, 0], [0, 1], [1, 0], [1, 1] ] ), their labels (supervised Leaning, In XOR case [0, 1, 1, 0] ), epochos i.e the number of time I want to pass my data from model, learning rate -> any small value in my case I take it as 0.1.\n",
        "\n",
        "6. for backword propegation weights are updated, i.e W`= W − η⋅ ∂L/∂y(predictd) . ∂y(predictd)/ ∂h1 . ∂h1/∂W\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cEaluLfq5nqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.w1 = random.uniform(-1, 1)\n",
        "        self.w2 = random.uniform(-1, 1)\n",
        "        self.b1 = random.uniform(-1, 1)\n",
        "        self.w3 = random.uniform(-1, 1)\n",
        "        self.w4 = random.uniform(-1, 1)\n",
        "        self.b2 = random.uniform(-1, 1)\n",
        "        self.w5 = random.uniform(-1, 1)\n",
        "        self.w6 = random.uniform(-1, 1)\n",
        "        self.b3 = random.uniform(-1, 1)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "\n",
        "    def derivative_sigmoid(self, x):\n",
        "        fx = self.sigmoid(x)\n",
        "        return fx * (1 - fx)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        h1 = self.sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "        h2 = self.sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "        o1 = self.sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "        return o1\n",
        "\n",
        "    def train(self, data, labels, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x, y_true in zip(data, labels):\n",
        "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "                h1 = self.sigmoid(sum_h1)\n",
        "\n",
        "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "                h2 = self.sigmoid(sum_h2)\n",
        "\n",
        "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "                o1 = self.sigmoid(sum_o1)\n",
        "                y_pred = o1\n",
        "\n",
        "\n",
        "\n",
        "                loss = (y_true - y_pred) ** 2\n",
        "                total_loss += loss\n",
        "               # rmse = math.sqrt(total_loss / len(data))\n",
        "\n",
        "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "                d_ypred_d_w5 = h1 * self.derivative_sigmoid(sum_o1)\n",
        "                d_ypred_d_w6 = h2 * self.derivative_sigmoid(sum_o1)\n",
        "                d_ypred_d_b3 = self.derivative_sigmoid(sum_o1)\n",
        "\n",
        "                d_ypred_d_h1 = self.w5 * self.derivative_sigmoid(sum_o1)\n",
        "                d_ypred_d_h2 = self.w6 * self.derivative_sigmoid(sum_o1)\n",
        "\n",
        "                d_h1_d_w1 = x[0] * self.derivative_sigmoid(sum_h1)\n",
        "                d_h1_d_w2 = x[1] * self.derivative_sigmoid(sum_h1)\n",
        "                d_h1_d_b1 = self.derivative_sigmoid(sum_h1)\n",
        "\n",
        "                d_h2_d_w3 = x[0] * self.derivative_sigmoid(sum_h2)\n",
        "                d_h2_d_w4 = x[1] * self.derivative_sigmoid(sum_h2)\n",
        "                d_h2_d_b2 = self.derivative_sigmoid(sum_h2)\n",
        "\n",
        "                self.w1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "                self.w2 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "                self.b1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "                self.w3 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "                self.w4 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "                self.b2 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "                self.w5 -= learning_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "                self.w6 -= learning_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "                self.b3 -= learning_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "           # print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(data)}')\n"
      ],
      "metadata": {
        "id": "6IiE8qak30I1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is trained and result will be assigned to a veriable network."
      ],
      "metadata": {
        "id": "akxoeweaFMik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = NeuralNetwork()"
      ],
      "metadata": {
        "id": "BvL0cdMiFLL3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using above defined fnction, I will train my model on XOR data, passing other parameters as well, I chooose epoch as 1000 rounds for better trained and learing rate as 0.1 for fast learning (generally we take it as 0.01).\n",
        "\n",
        "\n",
        "*   I started with error : 0.0009426991191646666 in first stage, which keep on reducing by updating weights through backword propegation.\n",
        "*   Final Error: 0.0008247080119330441\n",
        "* improvement: ~12%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qC7BRCP4FhPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "labels = [0, 1, 1, 0]\n",
        "\n",
        "network.train(data, labels, epochs=1000, learning_rate=0.1)"
      ],
      "metadata": {
        "id": "Xekr9NtPFZ0H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is trained on XOR data, Now lets test it."
      ],
      "metadata": {
        "id": "gWrF0WwRGLp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Deep Learning Assigmnet 1')\n",
        "print('Submitted by Majid khan')\n",
        "print('--------------------------- \\n')\n",
        "input1 = int(input(\"Enter the first binary input: \"))\n",
        "input2 = int(input(\"Enter the second binary input: \"))\n",
        "print('--------------------------- \\n')\n",
        "print(f\"XOR({input1}, {input2}) = {round(network.feedforward([input1, input2]))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1a-rw8rGSdS",
        "outputId": "4a477b93-35b9-4c3a-ae96-7b5fa0afaef4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Learning Assigmnet 1\n",
            "Submitted by Majid khan\n",
            "--------------------------- \n",
            "\n",
            "Enter the first binary input: 1\n",
            "Enter the second binary input: 1\n",
            "--------------------------- \n",
            "\n",
            "XOR(1, 1) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wWTjyFXRFKzq"
      }
    }
  ]
}